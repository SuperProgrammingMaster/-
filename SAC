{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMyaysN+4oekAVr0E2jvZVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuperProgrammingMaster/-/blob/main/SAC\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "def create_log_gaussian(mean, log_std, t):\n",
        "    quadratic = -((0.5 * (t - mean) / (log_std.exp())).pow(2))\n",
        "    l = mean.shape\n",
        "    log_z = log_std\n",
        "    z = l[-1] * math.log(2 * math.pi)\n",
        "    log_p = quadratic.sum(dim=-1) - log_z.sum(dim=-1) - 0.5 * z\n",
        "    return log_p\n",
        "\n",
        "def logsumexp(inputs, dim=None, keepdim=False):\n",
        "    if dim is None:\n",
        "        inputs = inputs.view(-1)\n",
        "        dim = 0\n",
        "    s, _ = torch.max(inputs, dim=dim, keepdim=True)\n",
        "    outputs = s + (inputs - s).exp().sum(dim=dim, keepdim=True).log()\n",
        "    if not keepdim:\n",
        "        outputs = outputs.squeeze(dim)\n",
        "    return outputs\n",
        "\n",
        "def soft_update(target, source, tau):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "\n",
        "def hard_update(target, source):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(param.data)"
      ],
      "metadata": {
        "id": "6iyQmFCbcM_W"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def add(self, state, action, reward, next_state,done):\n",
        "        self.buffer.append((state, action, reward, next_state,done))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def get_batch(self, batch_size):\n",
        "        batch_size = min(batch_size, len(self.buffer))\n",
        "        data = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # üî• ÏûêÎèô ÏÑ†ÌÉù\n",
        "\n",
        "        state = torch.tensor(np.array([x[0] for x in data]), dtype=torch.float32)\n",
        "        action = torch.tensor(np.array([x[1] for x in data]), dtype=torch.float32)\n",
        "        reward = torch.tensor(np.array([x[2] for x in data]), dtype=torch.float32)\n",
        "        next_state = torch.tensor(np.array([x[3] for x in data]), dtype=torch.float32)\n",
        "        done = torch.tensor(np.array([x[4] for x in data]), dtype=torch.float32)\n",
        "\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_g6ExCgBdJrJ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch.distributions import Normal\n",
        "\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_inputs, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim,1)\n",
        "        self.relu = torch.relu\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = state\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim,1)\n",
        "\n",
        "        self.fc4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
        "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc6 = nn.Linear(hidden_dim,1)\n",
        "        self.relu = torch.relu\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        xu = torch.cat((state,action),dim=1)\n",
        "\n",
        "        x = self.relu(self.fc1(xu))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        x2 = self.relu(self.fc4(xu))\n",
        "        x2 = self.relu(self.fc5(x2))\n",
        "        x2 = self.fc6(x2)\n",
        "        return x, x2\n",
        "\n",
        "\n",
        "class GausianPolicy(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
        "        super(GausianPolicy, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_inputs, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.mean_fc = nn.Linear(hidden_dim, num_actions)\n",
        "        self.log_std_fc = nn.Linear(hidden_dim, num_actions)\n",
        "        self.relu = torch.relu\n",
        "\n",
        "        if action_space is None:\n",
        "            self.register_buffer('action_scale', torch.tensor(1.).to(\"cuda\"))\n",
        "            self.register_buffer('action_bias', torch.tensor(0.).to(\"cuda\"))\n",
        "        else:\n",
        "            self.register_buffer('action_scale', torch.FloatTensor((action_space.high - action_space.low) / 2.).to(\"cuda\"))\n",
        "            self.register_buffer('action_bias', torch.FloatTensor((action_space.high + action_space.low) / 2.).to(\"cuda\"))\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.relu(self.fc1(state))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        mean = self.mean_fc(x)\n",
        "        log_std = self.log_std_fc(x)\n",
        "        log_std = torch.clamp(log_std, min=-20, max=2)\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state):\n",
        "        mean, log_std = self.forward(state)\n",
        "        std = log_std.exp()\n",
        "        distribution = Normal(mean, std)\n",
        "        x_t = distribution.rsample()\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "\n",
        "        log_probs = distribution.log_prob(x_t)\n",
        "        log_probs -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
        "        log_probs = log_probs.sum(1, keepdim=True)\n",
        "\n",
        "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
        "        return action, log_probs, mean\n",
        "\n"
      ],
      "metadata": {
        "id": "_YhORRLudMx6"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch.distributions import Normal\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "class SAC:\n",
        "    def __init__(self, num_inputs, num_actions, action_space=None):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # üî• ÏûêÎèôÏúºÎ°ú GPU/CPU ÏÑ†ÌÉù\n",
        "\n",
        "        self.hidden_dim = 128\n",
        "        self.alpha = 0.2\n",
        "        self.target_entropy = -torch.prod(torch.tensor(action_space.shape)).item()\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "\n",
        "        self.critic_lr = 0.0001\n",
        "        self.critic_target_lr = 0.0001\n",
        "        self.policy_lr = 0.0001\n",
        "\n",
        "        self.critic = QNetwork(num_inputs, num_actions, self.hidden_dim)\n",
        "        self.critic_optim = Adam(self.critic.parameters(), lr=self.critic_lr)\n",
        "\n",
        "        self.critic_target = QNetwork(num_inputs, num_actions, self.hidden_dim)\n",
        "        hard_update(self.critic_target, self.critic)\n",
        "\n",
        "        self.policy = GausianPolicy(num_inputs, action_space.shape[0], self.hidden_dim, action_space)\n",
        "        self.policy_optim = Adam(self.policy.parameters(), lr=self.critic_target_lr)\n",
        "\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)  # üî• device Ï†ÅÏö©\n",
        "        self.alpha_optim = Adam([self.log_alpha], lr=self.policy_lr)\n",
        "\n",
        "\n",
        "\n",
        "    def get_action(self,state, evaluate):\n",
        "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
        "        if not evaluate:\n",
        "            action,_,_ = self.policy.sample(state)\n",
        "        else:\n",
        "            _,_,action = self.policy.sample(state)\n",
        "        return action.detach().cpu().numpy()[0]\n",
        "    def update_parameters(self, memory, batch_size, updates):\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.get_batch(batch_size)\n",
        "\n",
        "        # üî• GPU/CPUÎ°ú Îç∞Ïù¥ÌÑ∞ Ïù¥Îèô\n",
        "        state_batch = state_batch\n",
        "        action_batch = action_batch\n",
        "        reward_batch = reward_batch.float()\n",
        "        next_state_batch = next_state_batch\n",
        "        mask_batch = mask_batch.float()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
        "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
        "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
        "            next_q_value = reward_batch.unsqueeze(-1) + mask_batch.unsqueeze(-1) * self.gamma * min_qf_next_target\n",
        "            next_q_value = next_q_value.float()\n",
        "\n",
        "        # Q Ìï®Ïàò Í∞í Í≥ÑÏÇ∞\n",
        "        qf1, qf2 = self.critic(state_batch, action_batch)\n",
        "\n",
        "        # MSE Loss Í≥ÑÏÇ∞\n",
        "        qf1_loss = F.mse_loss(qf1, next_q_value.detach())\n",
        "        qf2_loss = F.mse_loss(qf2, next_q_value.detach())\n",
        "        qf_loss = qf1_loss + qf2_loss\n",
        "\n",
        "        # Critic ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        self.critic_optim.zero_grad()\n",
        "        qf_loss.backward()\n",
        "        self.critic_optim.step()\n",
        "    def save_checkpoint(self, env_name, suffix=\"\", ckpt_path=None):\n",
        "        if not os.path.exists('checkpoints/'):\n",
        "            os.makedirs('checkpoints/')\n",
        "        if ckpt_path is None:\n",
        "            ckpt_path = \"checkpoints/sac_checkpoint_{}_{}\".format(env_name, suffix)\n",
        "        print('Saving models to {}'.format(ckpt_path))\n",
        "        torch.save({'policy_state_dict': self.policy.state_dict(),\n",
        "                    'critic_state_dict': self.critic.state_dict(),\n",
        "                    'critic_target_state_dict': self.critic_target.state_dict(),\n",
        "                    'critic_optimizer_state_dict': self.critic_optim.state_dict(),\n",
        "                    'policy_optimizer_state_dict': self.policy_optim.state_dict()}, ckpt_path)\n",
        "\n",
        "    # Load model parameters\n",
        "    def load_checkpoint(self, ckpt_path, evaluate=False):\n",
        "        print('Loading models from {}'.format(ckpt_path))\n",
        "        if ckpt_path is not None:\n",
        "            checkpoint = torch.load(ckpt_path)\n",
        "            self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
        "            self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])\n",
        "            self.critic_optim.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
        "            self.policy_optim.load_state_dict(checkpoint['policy_optimizer_state_dict'])\n",
        "\n",
        "            if evaluate:\n",
        "                self.policy.eval()\n",
        "                self.critic.eval()\n",
        "                self.critic_target.eval()\n",
        "            else:\n",
        "                self.policy.train()\n",
        "                self.critic.train()\n",
        "                self.critic_target.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cFCRkaqfdQK5"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.distributions import Normal\n",
        "\n",
        "\n",
        "reward_history = []\n",
        "episodes = 30000\n",
        "env = gym.make('MountainCarContinuous-v0') # , render_mode='human'\n",
        "agent = SAC(env.observation_space.shape[0], env.action_space.shape[0],env.action_space)\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "memory = ReplayBuffer(1000000,batch_size)\n",
        "\n",
        "\n",
        "updates = 0\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    if ((episode+1) % 20 == 0):\n",
        "        agent.save_checkpoint(\"MountainCarContinuous-v0\")\n",
        "    while not done:\n",
        "        evaleuate = False\n",
        "        if evaleuate:\n",
        "            print(\"ÌèâÍ∞ÄÎ™®Îìú ON\")\n",
        "            action = agent.get_action(state,True)\n",
        "        else:\n",
        "            action = agent.get_action(state,False)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action=action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "\n",
        "        if evaleuate:\n",
        "            continue\n",
        "        memory.add(state,action,reward,next_state,1-done)\n",
        "\n",
        "\n",
        "        if len(memory) >= batch_size:\n",
        "            # Number of updates per step in environment\n",
        "            for i in range(3):\n",
        "                agent.update_parameters(memory,batch_size,updates)\n",
        "                updates += 1\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Episode {episode} - Total Reward: {total_reward} - Evaluate : {evaleuate}\")\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "env.close()\n",
        "agent.save_checkpoint(\"MountainCarContinuous-v0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XOLexhr1dcLN",
        "outputId": "b36e8a88-14f5-4933-8569-28cb3ee85e10"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 - Total Reward: -31.88432182508157 - Evaluate : False\n",
            "Episode 1 - Total Reward: -33.329540345604364 - Evaluate : False\n",
            "Episode 2 - Total Reward: -32.675760086907616 - Evaluate : False\n",
            "Episode 3 - Total Reward: -32.29067202630778 - Evaluate : False\n",
            "Episode 4 - Total Reward: -32.11905160578733 - Evaluate : False\n",
            "Episode 5 - Total Reward: -34.07914365317124 - Evaluate : False\n",
            "Episode 6 - Total Reward: -34.14355236017328 - Evaluate : False\n",
            "Episode 7 - Total Reward: -33.02048762168092 - Evaluate : False\n",
            "Episode 8 - Total Reward: -32.59456196689363 - Evaluate : False\n",
            "Episode 9 - Total Reward: -32.18128528595842 - Evaluate : False\n",
            "Episode 10 - Total Reward: -34.413758219873515 - Evaluate : False\n",
            "Episode 11 - Total Reward: -31.743193583621306 - Evaluate : False\n",
            "Episode 12 - Total Reward: -32.45537020529724 - Evaluate : False\n",
            "Episode 13 - Total Reward: -34.40602424667353 - Evaluate : False\n",
            "Episode 14 - Total Reward: -33.58004560157055 - Evaluate : False\n",
            "Episode 15 - Total Reward: -31.298702508056586 - Evaluate : False\n",
            "Episode 16 - Total Reward: -33.982821036292904 - Evaluate : False\n",
            "Episode 17 - Total Reward: -33.468058341356375 - Evaluate : False\n",
            "Episode 18 - Total Reward: -33.68364663338483 - Evaluate : False\n",
            "Saving models to checkpoints/sac_checkpoint_MountainCarContinuous-v0_\n",
            "Episode 19 - Total Reward: -35.13720627658764 - Evaluate : False\n",
            "Episode 20 - Total Reward: -35.57801464292063 - Evaluate : False\n",
            "Episode 21 - Total Reward: -33.06048564467951 - Evaluate : False\n",
            "Episode 22 - Total Reward: -33.524946195112854 - Evaluate : False\n",
            "Episode 23 - Total Reward: -32.43493904730797 - Evaluate : False\n",
            "Episode 24 - Total Reward: -32.42636625731617 - Evaluate : False\n",
            "Episode 25 - Total Reward: -34.48909464641489 - Evaluate : False\n",
            "Episode 26 - Total Reward: -31.093321227323877 - Evaluate : False\n",
            "Episode 27 - Total Reward: -32.913012272224464 - Evaluate : False\n",
            "Episode 28 - Total Reward: -33.13696594022501 - Evaluate : False\n",
            "Episode 29 - Total Reward: -34.297617905507856 - Evaluate : False\n",
            "Episode 30 - Total Reward: -33.206210401868624 - Evaluate : False\n",
            "Episode 31 - Total Reward: -33.507887488266256 - Evaluate : False\n",
            "Episode 32 - Total Reward: -32.51009910006576 - Evaluate : False\n",
            "Episode 33 - Total Reward: -32.719425343775995 - Evaluate : False\n",
            "Episode 34 - Total Reward: -33.7012810530163 - Evaluate : False\n",
            "Episode 35 - Total Reward: -34.26857663802329 - Evaluate : False\n",
            "Episode 36 - Total Reward: -33.23784290384311 - Evaluate : False\n",
            "Episode 37 - Total Reward: -32.74772131783784 - Evaluate : False\n",
            "Episode 38 - Total Reward: -34.917479986695824 - Evaluate : False\n",
            "Saving models to checkpoints/sac_checkpoint_MountainCarContinuous-v0_\n",
            "Episode 39 - Total Reward: -33.44668299480659 - Evaluate : False\n",
            "Episode 40 - Total Reward: -32.066163543450386 - Evaluate : False\n",
            "Episode 41 - Total Reward: -33.07304100336678 - Evaluate : False\n",
            "Episode 42 - Total Reward: -32.567777065888656 - Evaluate : False\n",
            "Episode 43 - Total Reward: -31.6092340164653 - Evaluate : False\n",
            "Episode 44 - Total Reward: -34.24458425013994 - Evaluate : False\n",
            "Episode 45 - Total Reward: -33.45170922526418 - Evaluate : False\n",
            "Episode 46 - Total Reward: -32.3681361527283 - Evaluate : False\n",
            "Episode 47 - Total Reward: -33.67273849091362 - Evaluate : False\n",
            "Episode 48 - Total Reward: -31.44615431064287 - Evaluate : False\n",
            "Episode 49 - Total Reward: -34.02233944405728 - Evaluate : False\n",
            "Episode 50 - Total Reward: -32.421638500785384 - Evaluate : False\n",
            "Episode 51 - Total Reward: -33.62880882945717 - Evaluate : False\n",
            "Episode 52 - Total Reward: -32.020095282195584 - Evaluate : False\n",
            "Episode 53 - Total Reward: -32.313890145361086 - Evaluate : False\n",
            "Episode 54 - Total Reward: -33.27935789500214 - Evaluate : False\n",
            "Episode 55 - Total Reward: -32.6683175049555 - Evaluate : False\n",
            "Episode 56 - Total Reward: -32.73754395086834 - Evaluate : False\n",
            "Episode 57 - Total Reward: -33.82355775654562 - Evaluate : False\n",
            "Episode 58 - Total Reward: -32.83264279161283 - Evaluate : False\n",
            "Saving models to checkpoints/sac_checkpoint_MountainCarContinuous-v0_\n",
            "Episode 59 - Total Reward: -33.61205328496303 - Evaluate : False\n",
            "Episode 60 - Total Reward: -33.490622690464015 - Evaluate : False\n",
            "Episode 61 - Total Reward: -32.681245230464974 - Evaluate : False\n",
            "Episode 62 - Total Reward: -32.95358256261542 - Evaluate : False\n",
            "Episode 63 - Total Reward: -33.709605778157695 - Evaluate : False\n",
            "Episode 64 - Total Reward: -32.9475704116976 - Evaluate : False\n",
            "Episode 65 - Total Reward: -33.29487352120031 - Evaluate : False\n",
            "Episode 66 - Total Reward: -33.13763836806531 - Evaluate : False\n",
            "Episode 67 - Total Reward: -34.223532896327704 - Evaluate : False\n",
            "Episode 68 - Total Reward: -33.629382690781 - Evaluate : False\n",
            "Episode 69 - Total Reward: -33.7786142366373 - Evaluate : False\n",
            "Episode 70 - Total Reward: -33.067693098070656 - Evaluate : False\n",
            "Episode 71 - Total Reward: -33.246125251379894 - Evaluate : False\n",
            "Episode 72 - Total Reward: -35.37218149803493 - Evaluate : False\n",
            "Episode 73 - Total Reward: -32.33899966955769 - Evaluate : False\n",
            "Episode 74 - Total Reward: -34.022974494958284 - Evaluate : False\n",
            "Episode 75 - Total Reward: -30.71512908332631 - Evaluate : False\n",
            "Episode 76 - Total Reward: -34.89510394055301 - Evaluate : False\n",
            "Episode 77 - Total Reward: -33.21911928730447 - Evaluate : False\n",
            "Episode 78 - Total Reward: -34.07448398216085 - Evaluate : False\n",
            "Saving models to checkpoints/sac_checkpoint_MountainCarContinuous-v0_\n",
            "Episode 79 - Total Reward: -34.04577635727338 - Evaluate : False\n",
            "Episode 80 - Total Reward: -33.08185047289354 - Evaluate : False\n",
            "Episode 81 - Total Reward: -33.55107059362731 - Evaluate : False\n",
            "Episode 82 - Total Reward: -33.44435697222915 - Evaluate : False\n",
            "Episode 83 - Total Reward: -34.56203347199222 - Evaluate : False\n",
            "Episode 84 - Total Reward: -33.85663546377613 - Evaluate : False\n",
            "Episode 85 - Total Reward: -33.17246977550937 - Evaluate : False\n",
            "Episode 86 - Total Reward: -33.993967918760085 - Evaluate : False\n",
            "Episode 87 - Total Reward: -32.78235366049981 - Evaluate : False\n",
            "Episode 88 - Total Reward: -34.03686760597102 - Evaluate : False\n",
            "Episode 89 - Total Reward: -33.69916335627026 - Evaluate : False\n",
            "Episode 90 - Total Reward: -31.604147550090786 - Evaluate : False\n",
            "Episode 91 - Total Reward: -33.60019866898102 - Evaluate : False\n",
            "Episode 92 - Total Reward: -34.13115064562293 - Evaluate : False\n",
            "Episode 93 - Total Reward: -32.047186650139935 - Evaluate : False\n",
            "Episode 94 - Total Reward: -33.43800559325309 - Evaluate : False\n",
            "Episode 95 - Total Reward: -33.21168090108671 - Evaluate : False\n",
            "Episode 96 - Total Reward: -33.96633733767334 - Evaluate : False\n",
            "Episode 97 - Total Reward: -33.288432047509765 - Evaluate : False\n",
            "Episode 98 - Total Reward: -32.92014804967988 - Evaluate : False\n",
            "Saving models to checkpoints/sac_checkpoint_MountainCarContinuous-v0_\n",
            "Episode 99 - Total Reward: -31.186304248203612 - Evaluate : False\n",
            "Episode 100 - Total Reward: -32.43643359311351 - Evaluate : False\n",
            "Episode 101 - Total Reward: -33.54045026697717 - Evaluate : False\n",
            "Episode 102 - Total Reward: -33.08496319432811 - Evaluate : False\n",
            "Episode 103 - Total Reward: -34.388458983816285 - Evaluate : False\n",
            "Episode 104 - Total Reward: -32.24695477492979 - Evaluate : False\n",
            "Episode 105 - Total Reward: -32.66987998409932 - Evaluate : False\n",
            "Episode 106 - Total Reward: -33.00383671422335 - Evaluate : False\n",
            "Episode 107 - Total Reward: -31.545351208809286 - Evaluate : False\n",
            "Episode 108 - Total Reward: -32.720785666574955 - Evaluate : False\n",
            "Episode 109 - Total Reward: -32.63866637282734 - Evaluate : False\n",
            "Episode 110 - Total Reward: -33.54997793382464 - Evaluate : False\n",
            "Episode 111 - Total Reward: -33.74936780651256 - Evaluate : False\n",
            "Episode 112 - Total Reward: -33.082777235487974 - Evaluate : False\n",
            "Episode 113 - Total Reward: -32.19047874768476 - Evaluate : False\n",
            "Episode 114 - Total Reward: -31.289648456947376 - Evaluate : False\n",
            "Episode 115 - Total Reward: -32.605216798511734 - Evaluate : False\n",
            "Episode 116 - Total Reward: -33.43546837006057 - Evaluate : False\n",
            "Episode 117 - Total Reward: -35.77564881208087 - Evaluate : False\n",
            "Episode 118 - Total Reward: -32.62308011063023 - Evaluate : False\n",
            "Saving models to checkpoints/sac_checkpoint_MountainCarContinuous-v0_\n",
            "Episode 119 - Total Reward: -32.42939851960511 - Evaluate : False\n",
            "Episode 120 - Total Reward: -32.26188483013923 - Evaluate : False\n",
            "Episode 121 - Total Reward: -33.406530042904464 - Evaluate : False\n",
            "Episode 122 - Total Reward: -32.33744109621764 - Evaluate : False\n",
            "Episode 123 - Total Reward: -32.13289659054228 - Evaluate : False\n",
            "Episode 124 - Total Reward: -33.44661985325018 - Evaluate : False\n",
            "Episode 125 - Total Reward: -34.556359907915585 - Evaluate : False\n",
            "Episode 126 - Total Reward: -32.50264885012672 - Evaluate : False\n",
            "Episode 127 - Total Reward: -32.34340248518657 - Evaluate : False\n",
            "Episode 128 - Total Reward: -32.242795597346394 - Evaluate : False\n",
            "Episode 129 - Total Reward: -31.947182723132585 - Evaluate : False\n",
            "Episode 130 - Total Reward: -33.39067944653716 - Evaluate : False\n",
            "Episode 131 - Total Reward: -34.25519657156698 - Evaluate : False\n",
            "Episode 132 - Total Reward: -32.82925337502181 - Evaluate : False\n",
            "Episode 133 - Total Reward: -32.60041298266814 - Evaluate : False\n",
            "Episode 134 - Total Reward: -33.21115263051305 - Evaluate : False\n",
            "Episode 135 - Total Reward: -32.33967441688811 - Evaluate : False\n",
            "Episode 136 - Total Reward: -32.68094426440206 - Evaluate : False\n",
            "Episode 137 - Total Reward: -32.94081938887766 - Evaluate : False\n",
            "Episode 138 - Total Reward: -33.103214356113654 - Evaluate : False\n",
            "Saving models to checkpoints/sac_checkpoint_MountainCarContinuous-v0_\n",
            "Episode 139 - Total Reward: -34.11132202292728 - Evaluate : False\n",
            "Episode 140 - Total Reward: -33.03803625790527 - Evaluate : False\n",
            "Episode 141 - Total Reward: -33.34168016774843 - Evaluate : False\n",
            "Episode 142 - Total Reward: -33.49242762390909 - Evaluate : False\n",
            "Episode 143 - Total Reward: -33.6849324788038 - Evaluate : False\n",
            "Episode 144 - Total Reward: -33.6204334419038 - Evaluate : False\n",
            "Episode 145 - Total Reward: -32.06579803902883 - Evaluate : False\n",
            "Episode 146 - Total Reward: -32.781438121801344 - Evaluate : False\n",
            "Episode 147 - Total Reward: -31.681752821356266 - Evaluate : False\n",
            "Episode 148 - Total Reward: -32.877639189869534 - Evaluate : False\n",
            "Episode 149 - Total Reward: -32.26355053634748 - Evaluate : False\n",
            "Episode 150 - Total Reward: -32.46928940351109 - Evaluate : False\n",
            "Episode 151 - Total Reward: -32.80374315849664 - Evaluate : False\n",
            "Episode 152 - Total Reward: -34.28428922727522 - Evaluate : False\n",
            "Episode 153 - Total Reward: -31.231079655388356 - Evaluate : False\n",
            "Episode 154 - Total Reward: -33.843401739966005 - Evaluate : False\n",
            "Episode 155 - Total Reward: -33.134716558850855 - Evaluate : False\n",
            "Episode 156 - Total Reward: -33.8046938756245 - Evaluate : False\n",
            "Episode 157 - Total Reward: -32.54491947835053 - Evaluate : False\n",
            "Episode 158 - Total Reward: -32.88513028718544 - Evaluate : False\n",
            "Saving models to checkpoints/sac_checkpoint_MountainCarContinuous-v0_\n",
            "Episode 159 - Total Reward: -33.0498074188978 - Evaluate : False\n",
            "Episode 160 - Total Reward: -31.24400972203231 - Evaluate : False\n",
            "Episode 161 - Total Reward: -33.377303956188946 - Evaluate : False\n",
            "Episode 162 - Total Reward: -32.773806489969886 - Evaluate : False\n",
            "Episode 163 - Total Reward: -32.52731993856231 - Evaluate : False\n",
            "Episode 164 - Total Reward: -33.221449998621566 - Evaluate : False\n",
            "Episode 165 - Total Reward: -33.77984984908515 - Evaluate : False\n",
            "Episode 166 - Total Reward: -32.090076868646605 - Evaluate : False\n",
            "Episode 167 - Total Reward: -33.744762124055434 - Evaluate : False\n",
            "Episode 168 - Total Reward: -33.80806766683114 - Evaluate : False\n",
            "Episode 169 - Total Reward: -32.05846025835557 - Evaluate : False\n",
            "Episode 170 - Total Reward: -32.20938897341641 - Evaluate : False\n",
            "Episode 171 - Total Reward: -32.37646389017277 - Evaluate : False\n",
            "Episode 172 - Total Reward: -32.38377723221592 - Evaluate : False\n",
            "Episode 173 - Total Reward: -32.51573359493506 - Evaluate : False\n",
            "Episode 174 - Total Reward: -33.89919766071931 - Evaluate : False\n",
            "Episode 175 - Total Reward: -34.51371177888742 - Evaluate : False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-cb6e1e15147f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Number of updates per step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-9da51e7382e1>\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# üî• GPU/CPUÎ°ú Îç∞Ïù¥ÌÑ∞ Ïù¥Îèô\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-fbebe7e7006c>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# üî• ÏûêÎèô ÏÑ†ÌÉù\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    475\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mselected_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}